---
tags:
  - Y2018
  - "#CVPR"
  - "#Quantization"
link: https://arxiv.org/pdf/1712.05877.pdf
Main Author:
  - Bita Rouhani
Organization:
  - Microsoft
  - Meta
---
# Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
#Y2018 #CVPR #Quantization 

# Introduction

다양한 목적의 많은 인공지능 관련 연구는 효율성을 위해서 연산 속도를 개선하기 위한 여러 노력들을 해 왔다. 그 중 한 방향성은 [[MobileNet]], [[SqueezeNet]], [[ShuffleNet]], [[DenseNet]]과 같이 새로운 네트워크 구조를 제안해서 성능을 개선하려는 것이고, 다른 방향성은 [[TWN]], [[BNN]], [[XNOR-net]]과 같이 낮은 bit width을 사용해서 성능을 확보하려는 방식이다.

이전 연구들은 두 가지 점에서 부족한 부분이 있다.
1. 첫 번째는 적당한 모델을 사용하지 않았다는 점으로, [[AlexNet]], [[VGG]], [[GoogleNet]]의 경우 이미 네트워크 파라미터가 너무 많아서 압축을 많이 진행하더라도 성능 손해를 많이 안 볼 가능성이 크다. 따라서 [[MobileNet]]과 같이 이미 속도를 위해서 어느 정도 정확도를 손해 보고 있는 네트워크를 대상으로 실험하는 것이 바람직하다.
2. 두 번째는,  실제 하드웨어에서 확인 가능한 성능 개선에 대한 결과를 포함하고 있지 않다는 것이다. weight만 양자화할 때, device에 올라가는 메모리 크기만 고려되고, 실제 연산 효율성은 고려되지 않았다. 또한, 특정한 연산도 커스텀 하드웨어에서는 빠를 수 있지만, 기존 존재하는 하드웨어에서는 해당 연산이 연산 속도에서 오히려 손해를 볼 수 있다.

따라서, 이 논문에서는
- 양자화 방법
- 양자화 inference 프레임워크
- 양자화 training 프레임워크
- 유명 ARM CPU 대상으로 한 벤치마크
를 주 포인트로 내세움.

# 주요 포인트

## 양자화 추론 (Quantized Inference)
### 양자화 방식

정수 연산 (Integer Arithmetic)만을 이용해서 양자화된 값을 계산하고, [[Affine Mapping]]과 동일한 효과(S, Z는 상수)를 가짐 

$$r = 실제\ 값,\ q = 양자화된\ 값$$

$$r = S(q-Z)$$


이 연구에서는 각 텐서(activation, weight)마다 S와 Z를 따로따로 가지도록 설정되어 있음. *8비트 양자화*모드에서  q는 8비트로 양자화되고, bias 같은 거는 32비트 integer를 사용함. S(scale)은 적당한 양의 실수 값으로, floating-point로 표현됨. Z(zero-point)는 양자화된 값 q와 같은 데이터 타입으로, 실제 값인 r이 0이 될 수 있도록 조정해주는 역할을 수행함. 인공지능 연산에서 배열 주변에 0을 채우거나 할 때 효율적인 연산을 위해서 존재함. 
실제 데이터 타입을 정리하면 아래와 같을 것임.

```c++
template // e.g. QType=uint8
struct QuantizedBuffer {
  vector<QType> q; // the quantized values
  float S; // the scale
  QType Z; // the zero-point
};
```

### 정수 연산만 사용하는 행렬곱
$N \times N$ 행렬 $r_1$, $r_2$의 곱셈을 해서 $r_3$이 나온다고 했을 때 $r_\alpha^{(i,j)} (1\leq i,j \leq N)$ 로 각 행렬의 성분을 표시하고, $S_\alpha$, $Z_\alpha$를 양자화 파라미터라고 두면, 양자화된 행렬의 곱을 위에 있는 식에 따라서 양자화된 값으로 표현하면 다음과 같이 표시 가능하다.

$$S_3(q_3^{(i,k)}-Z_3)=\sum_{j=1}^N{S_1(q_1^{(i,j)}-Z_1)S_2(q_2^{(j,k)}-Z_2)}$$

가 되고, 이걸 다시 표현하면 

$$q_3^{(i,k)}=Z_3+M\sum_{j=1}^N{(q_1^{(i,j)}-Z_1)(q_2^{(j,k)}-Z_2)}$$

에서 $M:=\frac{S_1S_2}{S_3}$로 표현되고, 따라서 정수연산이 아닌 건 M을 곱해주는 것 밖에 없게 된다.

여기서, 연구자들이 이 값은 (0, 1) 사이의 값을 가진다는 것을 경험에 의해 찾아냈고, 따라서 일반적인 형태인 $M=2^{-n}M_0 , (0.5\leq M_0<1)$로 표시할 수 있다. 이렇게 되면 int32 데이터형 기준으로 $M_0$를 표현하면 $2^{31}M_0$로 표현 가능하며 (정수형으로 처리하기 위해서), $M_0$의 값이 1/2보다 크기 때문에 언제나 위의 값은 $2^{30}$의 값을 가지고, 따라서 최소 30bit 이상의 유의미한 값을 가질 수 있다. 그리고, $2^{-n}$을 곱하는 것은 효율적인 비트 시프트 연산으로 처리할 수 있고, 전체 프로세스를 정수 곱셈 및 덧셈 연산만 사용해서 진행할 수 있게 된다. 그런즉, 효율적인 속도로 연산을 할 수 있다는 말이 된다! 🧐
### Zero-point($Z$)의 효율적인 처리 방법
위 연산에서 $2N^3$ 개수의 뺄셈을 해야 하는데, 그걸 하지 않고 가능한 효율적으로 진행하려면, 위 연산 계산식을 아래처럼 풀어서 쓸 수 있고,

$$q_3^{(i,k)}=Z_3+M(NZ_1Z_2-Z_1a_2^{(k)}-Z_2\bar{a}_1^{(i)}+\sum_{j=1}^N{q_1^{(i,j)}q_2^{(j,k)}}$$

$$a_2^{(k)}:=\sum_{j=1}^Nq_2^{(j,k)}+\bar{a}_1^{(i)}:=\sum_{j=1}^Nq_1^{i,j}$$

 식이 복잡해 보이지만 그냥 행렬 곱셈을 원소별로 풀어냈지만, $Z_1$과 $q_2$곱한 것과 $Z_2$와 $q_1$을 곱한 것을 따로 빼낸 것 뿐. 그래서 위 연산에서 $a_2^{(k)}, \bar{a}_1^{(i)}$는 각각 $N$개만 연산이 있으니까 총 덧셈 개수는 $2N^2$개라고 할 수 있고, 나머지 $q_1$과 $q_2$를 곱하는데 $2N^3$개의 연산이 들어간다. 그래서 Zero-point를 계산하는 과정은 큰 연산 부담이 없다.

### 일반적인 fused 레이어 구현


## 시뮬레이션된 양자화를 이용한 학습

